{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import os\n",
    "os.environ['KERAS_BACKEND'] = 'theano'\n",
    "import keras\n",
    "from keras import backend as K\n",
    "from keras import initializations\n",
    "from keras.regularizers import l1, l2, l1l2\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers.core import Dense, Lambda, Activation\n",
    "from keras.layers import Embedding, Input, Dense, merge, Reshape,  Flatten, Dropout\n",
    "from keras.optimizers import Adagrad, Adam, SGD, RMSprop\n",
    "from time import time\n",
    "import sys\n",
    "import argparse\n",
    "import csv\n",
    "import pandas as pd\n",
    "from math import ceil\n",
    "from sklearn.preprocessing import OneHotEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug=1\n",
    "rating_size=5 #used in one hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_normal(shape, name=None):\n",
    "    return initializations.normal(shape, scale=0.01, name=name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(num_users, num_items, mf_dim=10, layers=[10], reg_layers=[0], reg_mf=0):\n",
    "    assert len(layers) == len(reg_layers)\n",
    "    num_layer = len(layers) #Number of layers in the MLP\n",
    "    # Input variables\n",
    "    user_input = Input(shape=(1,), dtype='int32', name = 'user_input')\n",
    "    item_input = Input(shape=(1,), dtype='int32', name = 'item_input')\n",
    "    \n",
    "    # Embedding layer\n",
    "    MF_Embedding_User = Embedding(input_dim = num_users, output_dim = mf_dim, name = 'mf_embedding_user',\n",
    "                                  init = init_normal, W_regularizer = l2(reg_mf), input_length=1)\n",
    "    MF_Embedding_Item = Embedding(input_dim = num_items, output_dim = mf_dim, name = 'mf_embedding_item',\n",
    "                                  init = init_normal, W_regularizer = l2(reg_mf), input_length=1)   \n",
    "    if(debug):\n",
    "        print(\"layers values {}\".format(layers[0]))\n",
    "    MLP_Embedding_User = Embedding(input_dim = num_users, output_dim = int(layers[0]/2), name = \"mlp_embedding_user\",\n",
    "                                  init = init_normal, W_regularizer = l2(reg_layers[0]), input_length=1)\n",
    "    MLP_Embedding_Item = Embedding(input_dim = num_items, output_dim = int(layers[0]/2), name = 'mlp_embedding_item',\n",
    "                                  init = init_normal, W_regularizer = l2(reg_layers[0]), input_length=1)   \n",
    "    \n",
    "    # MF part\n",
    "    MF_Embedding_User(user_input)\n",
    "    mf_user_latent = Flatten()(MF_Embedding_User(user_input))\n",
    "    mf_item_latent = Flatten()(MF_Embedding_Item(item_input))\n",
    "    mf_vector = merge([mf_user_latent, mf_item_latent], mode = 'mul') # element-wise multiply\n",
    "\n",
    "    # MLP part \n",
    "    mlp_user_latent = Flatten()(MLP_Embedding_User(user_input))\n",
    "    mlp_item_latent = Flatten()(MLP_Embedding_Item(item_input))\n",
    "    mlp_vector = merge([mlp_user_latent, mlp_item_latent], mode = 'concat')\n",
    "    for idx in range(1, num_layer):\n",
    "        layer = Dense(layers[idx], W_regularizer= l2(reg_layers[idx]), activation='relu', name=\"layer%d\" %idx)\n",
    "        mlp_vector = layer(mlp_vector)\n",
    "\n",
    "    predict_vector = merge([mf_vector, mlp_vector], mode = 'concat')\n",
    "    \n",
    "    # Final prediction layer\n",
    "    prediction=Dense(1, activation='relu',init='lecun_uniform', name = \"prediction\")(predict_vector)\n",
    "    \n",
    "\n",
    "    \n",
    "    model = Model(input=[user_input, item_input], \n",
    "                  output=prediction)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_instances(train):\n",
    "    user_input, item_input, labels = [],[],[]\n",
    "    num_users = train.shape[0]\n",
    "    for (u, i) in train.keys():\n",
    "        # positive instance\n",
    "        user_input.append(u)\n",
    "        item_input.append(i)\n",
    "        labels.append(train[u,i])\n",
    "    return user_input, item_input, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataset(train_file_name,test_file_name,ratings_file_name):\n",
    "    train_df=pd.read_csv(train_file_name)\n",
    "    test_df=pd.read_csv(test_file_name)\n",
    "    ratings_file=pd.read_csv(ratings_file_name)\n",
    "    movies=ratings_file['movieId'].unique()\n",
    "    num_items=len(movies)\n",
    "    num_users=len(ratings_file['userId'].unique())\n",
    "    # movies_to_index={}\n",
    "    # for i in range(len(movies)):\n",
    "    #     movies_to_index[movies[i]]=i\n",
    "    # index_to_movies=movies\n",
    "    if(debug):\n",
    "        print(\"In read dataset function...Number of items {}\".format(num_items))\n",
    "        print(\"In read dataset function...Number of users {}\".format(num_users))\n",
    "    trainMatrix= sp.dok_matrix((num_users+1,num_items+1),dtype=np.float32)\n",
    "    for i in range (len(train_df)):\n",
    "        current_user_num=train_df['userId'][i]\n",
    "        current_item_num=train_df['movieId'][i]\n",
    "        # current_item_num=movies_to_index[current_item_num]\n",
    "        #print(current_user_num)\n",
    "        #print(current_item_num)\n",
    "        trainMatrix[current_user_num,current_item_num]=train_df['rating'][i]\n",
    "    testRatings=[]\n",
    "    testLabels=[]\n",
    "    for i in range (len(test_df)):\n",
    "        current_user_num=test_df['userId'][i]\n",
    "        current_item_num=test_df['movieId'][i]\n",
    "        # current_item_num=movies_to_index[current_item_num]\n",
    "        testRatings.append([current_user_num,current_item_num])  \n",
    "        testLabels.append(test_df['rating'][i])\n",
    "    if(debug):\n",
    "        print(\"dataset generation completed\")\n",
    "    return num_users,num_items,trainMatrix,testRatings,testLabels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model,user_input,item_input,labels, testRatings,testLabels):\n",
    "    #metric MAE\n",
    "    #for train\n",
    "    print(\"RMSE,MAE on train set: {}\".format(model.evaluate(x=[np.array(user_input),np.array(item_input)],y=np.array(labels),verbose=0)) )\n",
    "    testRatingUsers=np.array(testRatings)[:,0]\n",
    "    testRatingItems=np.array(testRatings)[:,1]\n",
    "    print(\"RMSE,MAE on test set: {}\".format(model.evaluate(x=[testRatingUsers,testRatingItems],y=np.array(testLabels),verbose=0)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In read dataset function...Number of items 9724\n",
      "In read dataset function...Number of users 610\n",
      "dataset generation completed\n",
      "Load data done [5.4 s]. #user=610, #item=9724, #train=80669, #test=20167\n",
      "layers values 64\n",
      "Epoch 1/1\n",
      "80669/80669 [==============================] - 7s - loss: 11.3942 - mean_absolute_error: 3.2003     \n",
      "RMSE,MAE on train set: [7.671884774006136, 2.5743324053120666]\n",
      "RMSE,MAE on test set: [7.8911848139643475, 2.6131724697469974]\n",
      "Epoch 1/1\n",
      "80669/80669 [==============================] - 7s - loss: 3.1408 - mean_absolute_error: 1.4666     - ET\n",
      "Epoch 1/1\n",
      "80669/80669 [==============================] - 8s - loss: 0.9287 - mean_absolute_error: 0.7563     - ETA: 0s - loss: 0.9356 - mean_ab - ETA: 0s - loss: 0.9311 - mean_absolute_err\n",
      "RMSE,MAE on train set: [0.8198424077078034, 0.7049304804157757]\n",
      "RMSE,MAE on test set: [1.1230725888241357, 0.8477608562003475]\n",
      "Epoch 1/1\n",
      "80669/80669 [==============================] - 8s - loss: 0.7936 - mean_absolute_error: 0.6911     \n",
      "Epoch 1/1\n",
      "80669/80669 [==============================] - 9s - loss: 0.7375 - mean_absolute_error: 0.6622     \n",
      "RMSE,MAE on train set: [0.7030258622404192, 0.6441814227954873]\n",
      "RMSE,MAE on test set: [1.057832613353152, 0.8188962694149435]\n",
      "Epoch 1/1\n",
      "80669/80669 [==============================] - 10s - loss: 0.7054 - mean_absolute_error: 0.6452    \n",
      "Epoch 1/1\n",
      "80669/80669 [==============================] - 9s - loss: 0.6845 - mean_absolute_error: 0.6337     \n",
      "RMSE,MAE on train set: [0.6606441069305269, 0.6202708856293764]\n",
      "RMSE,MAE on test set: [1.0742547480267466, 0.8226110258241682]\n",
      "Epoch 1/1\n",
      "80669/80669 [==============================] - 9s - loss: 0.6699 - mean_absolute_error: 0.6254     - ETA: 0s - loss: 0.6698 - mean_absolute_e\n",
      "Epoch 1/1\n",
      "80669/80669 [==============================] - 9s - loss: 0.6590 - mean_absolute_error: 0.6194     - ETA: 0s - loss: 0.6580 - mean_absolute_error:  - ETA: 0s - loss: 0.6589 - mean_absolute_error:\n",
      "RMSE,MAE on train set: [0.6383909770395761, 0.6074220647208993]\n",
      "RMSE,MAE on test set: [1.1124557013412233, 0.8350144089089924]\n",
      "Epoch 1/1\n",
      "60160/80669 [=====================>........] - ETA: 2s - loss: 0.6452 - mean_absolute_error: 0.6112"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-129-e25918a5f975>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     63\u001b[0m         hist = model.fit([np.array(user_input), np.array(item_input)], #input\n\u001b[1;32m     64\u001b[0m                          \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m# labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m                          batch_size=batch_size, nb_epoch=1, verbose=1, shuffle=True)\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0mt2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, nb_epoch, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight)\u001b[0m\n\u001b[1;32m   1105\u001b[0m                               \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1106\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1107\u001b[0;31m                               callback_metrics=callback_metrics)\n\u001b[0m\u001b[1;32m   1108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, nb_epoch, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics)\u001b[0m\n\u001b[1;32m    823\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 825\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    826\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/theano_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    655\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    656\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 657\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    658\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    659\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/theano/compile/function_module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    857\u001b[0m         \u001b[0mt0_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'position_of_error'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "#################### Arguments ####################\n",
    "\n",
    "    num_epochs = 100    \n",
    "    batch_size = 256\n",
    "    mf_dim = 8\n",
    "    layers = [64,32,16,8]\n",
    "    reg_mf = 0\n",
    "    reg_layers = [0,0,0,0]\n",
    "    num_negatives = 4\n",
    "    learning_rate = 1e-4\n",
    "    learner = 'adam'\n",
    "    verbose = 1\n",
    "    mf_pretrain = ''\n",
    "    mlp_pretrain = ''\n",
    "    path='Data/' \n",
    "    test_file_name=\"test12.csv\"\n",
    "    train_file_name=\"train12.csv\"\n",
    "    ratings_file_name=\"ratings.csv\" \n",
    "    topK = 10\n",
    "    evaluation_threads = 1#mp.cpu_count()\n",
    "    #model_out_file = 'Pretrain/%s_NeuMF_%d_%s_%d.h5' %(dataset, mf_dim, layers, time())\n",
    "    # Loading data\n",
    "    t1 = time()\n",
    "    #dataset = Dataset(path + dataset)\n",
    "    #train, testRatings, testNegatives = dataset.trainMatrix, dataset.testRatings, dataset.testNegatives\n",
    "    num_users, num_items,train,testRatings,testLabels = read_dataset(path+train_file_name,path+test_file_name,path+ratings_file_name)\n",
    "    print(\"Load data done [%.1f s]. #user=%d, #item=%d, #train=%d, #test=%d\" \n",
    "          %(time()-t1, num_users, num_items, train.nnz, len(testRatings)))\n",
    "    \n",
    "    # Build model\n",
    "    model = get_model(int(num_users+1), int(num_items+1), mf_dim, layers, reg_layers, reg_mf)\n",
    "\n",
    "    model.compile(optimizer=Adam(lr=learning_rate), loss= ['mse'], metrics=['mean_absolute_error'])\n",
    "\n",
    "\n",
    "        \n",
    "    # Training model\n",
    "    for epoch in range(num_epochs):\n",
    "        t1 = time()\n",
    "        # Generate training instances\n",
    "        user_input, item_input, labels = get_train_instances(train)\n",
    "        \n",
    "        # Training\n",
    "        hist = model.fit([np.array(user_input), np.array(item_input)], #input\n",
    "                         np.array(labels), # labels \n",
    "                         batch_size=batch_size, nb_epoch=1, verbose=1, shuffle=True)\n",
    "        t2 = time()\n",
    "        \n",
    "        # Evaluation\n",
    "        if epoch %2 == 0:\n",
    "            evaluate_model(model,user_input,item_input,labels, testRatings,testLabels)\n",
    "\n",
    "    print(\"End. Best Iteration %d:  HR = %.4f, NDCG = %.4f. \" %(best_iter, best_hr, best_ndcg))\n",
    "    if out > 0:\n",
    "        print(\"The best NeuMF model is saved to %s\" %(model_out_file))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
